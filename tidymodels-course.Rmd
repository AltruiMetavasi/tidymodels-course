---
title: "Introduction to tidymodels"
author: "R. Dimas Bagas Herlambang"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  revealjs::revealjs_presentation:
    theme: white
    highlight: haddock
    center: true
    progress: true
    incremental: false
    css: assets/css/style.css
    reveal_options:
      slideNumber: true
      previewLinks: true
---

```{r settings-env, include=FALSE}
# clean the environment
rm(list = ls())
```

```{r settings-chunk, include=FALSE}
# knit settings
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.asp = 0.5625,
  fig.align = "center",
  out.width = "85%",
  collapse = TRUE,
  comment = "#>"
)
```

```{r settings-libs, include=FALSE}
# import libs
library(plotly)
library(randomForest)
library(ranger)
library(tidyverse)
library(tidymodels)
```

```{r datasets, include=FALSE}
# import additional libs
library(lubridate)

# prepare example datasets
attrition <- read_csv("data/attrition.csv")
```

# Disclaimer

---

```{r disclaimer, echo=FALSE,out.width="45%", fig.align="left"}
knitr::include_graphics("assets/img/disclaimer/logo.png")
```

<br>

The following presentation is produced by the team at [Algoritma](https://algorit.ma) for its internal training This presentation is intended for a restricted audience only. It may not be reproduced, distributed, translated or adapted in any form outside these individuals and organizations without permission.

# Outline

## Why `tidymodels` Matters?

* Things we think we're doing it right
* Things we never think we could do it better

## Setting the Cross-Validation Scheme using `rsample`

* Rethinking: Why we need validation?
* Tidy way to `rsample`-ing your dataset

## Data Preprocess using `recipes`

* Rethinking: How we should treat train and test?
* Reproducible preprocess `recipes`

## Model Fitting using `parsnip`

* Rethinking: How many machine learning packages you used?
* One vegeta.. I mean package to rule them all: `parsnip`

## Model Evaluation using `yardstick`

* Rethinking: How we measure the goodness of our model?
* It's always good to bring your own `yardstick`

## Dive Deeper: Summing-up the Baseline Workflow

## Intermediate Workflow: Hyperparameter Tuning

# Why `tidymodels` Matters?

## Things we think we're doing it right

---

### Sample splitting

```{r}
# set seed
set.seed(100)

# train rows
in_train <- sample(1:nrow(attrition), nrow(attrition) * 0.8)

# check target distribution in train
prop.table(table(attrition$attrition[in_train]))

# check target distribution in test
prop.table(table(attrition$attrition[-in_train]))
```


---

### Numeric scaling

```{r}
# scale age in full dataset
age_scaled <- scale(attrition$age)

# check mean and standard deviation
attr(age_scaled, "scaled:center")
attr(age_scaled, "scaled:scale")

# scale age in train dataset
age_train_scaled <- scale(attrition$age[in_train])

# check mean and standard deviation
attr(age_train_scaled, "scaled:center")
attr(age_train_scaled, "scaled:scale")

# scale age in test dataset
age_test_scaled <- scale(attrition$age[-in_train])

# check mean and standard deviation
attr(age_test_scaled, "scaled:center")
attr(age_test_scaled, "scaled:scale")
```


## Things we never think we could do it better

---

### How we see model performance

```{r, echo=FALSE}
caret::confusionMatrix(as.factor(sample(attrition$attrition)), as.factor(attrition$attrition), positive = "yes")
```

---

### How we use Receiver Operating Curve

```{r, echo=FALSE}
example_data <- recipe(attrition ~ ., data = attrition) %>% 
  step_rm(employee_count, employee_number) %>%
  step_nzv(all_predictors()) %>% 
  step_string2factor(all_nominal(), -attrition) %>%
  step_string2factor(attrition, levels = c("yes", "no")) %>%
  prep(strings_as_factors = FALSE) %>% 
  juice()

logistic_reg(mode = "classification") %>% 
  set_engine("glm") %>% 
  fit(attrition ~ ., data = example_data) %>% 
  predict(example_data, type = "prob") %>% 
  bind_cols(example_data) %>% 
  select(truth = attrition, .pred_yes) %>% 
  roc_curve(truth, .pred_yes) %>% 
  autoplot()
```

# Setting the Cross-Validation Scheme using `rsample`

## Rethinking: Why we need validation?

## Tidy way to `rsample`-ing your dataset

---

[`rsample`](https://tidymodels.github.io/rsample/){target="_blank"} is part of `tidymodels` that could help us in splitting or resampling or machine learning dataset.

There are so many splitting and resampling approach provided by `rsample`--as you could see in its [full function references page](https://tidymodels.github.io/rsample/reference/index.html){target="_blank"}. In this introduction, we will use two most general function:

* [`initial_split()`](https://tidymodels.github.io/rsample/reference/initial_split.html){target="_blank"}:
    Simple train and test splitting.
* [`vfold_cv()`](https://tidymodels.github.io/rsample/reference/vfold_cv.html){target="_blank"}:
    k-fold splitting, with optional repetition argument.

---

### Initial splitting

---

Initial random sampling for splitting train and test could be done using `initial_split()`:

```{r}
# set seed
set.seed(100)

# create initial split
splitted <- initial_split(attrition, prop = 0.8)

# check train dataset
training(splitted)

# check test dataset
testing(splitted)
```

---

But sometimes, simple random sampling is not enough:

```{r}
# target distribution in full dataset
prop.table(table(attrition$attrition))

# target distribution in train dataset
prop.table(table(training(splitted)$attrition))

# target distribution in test dataset
prop.table(table(testing(splitted)$attrition))
```

---

This is where we need `strata` argument to use stratified random sampling:

```{r}
# set seed
set.seed(100)

# create stratified initial split
splitted <- initial_split(attrition, prop = 0.8, strata = "attrition")

# target distribution in full dataset
prop.table(table(attrition$attrition))

# target distribution in train dataset
prop.table(table(training(splitted)$attrition))

# target distribution in test dataset
prop.table(table(testing(splitted)$attrition))
```

---

### Cross-sectional resampling

---

To use k-fold validation splits--and optionally, with repetition--we could use `vfold_cv()`:

```{r}
# set seed
set.seed(100)

# create k-fold splits with repetition
resampled <- vfold_cv(attrition, v = 3, repeats = 2, strata = "attrition")

# quick check
resampled
```

---

Each train and test dataset are stored in `splits` column. We could use `analysis()` and `assessment()` to get the train and test dataset, consecutively:

```{r}
# check train dataset on an example split
analysis(resampled$splits[[1]])

# check test dataset on an example split
assessment(resampled$splits[[1]])
```

# Data Preprocess using `recipes`

## Rethinking: How do we should treat train and test?

## Reproducible preprocess `recipes`

---

[`recipes`](https://tidymodels.github.io/recipes/){target="_blank"} is part of `tidymodels` that could help us in making a reproducible data preprocess.

There are so many data preprocess approach provided by `recipes`--as you could see in its [full function references page](https://tidymodels.github.io/recipes/reference/index.html){target="_blank"}. In this introduction, we will use several preprocess steps related to our example dataset.

---

There are several steps that we could apply to our dataset--some are very fundamental and sometimes mandatory, but some are also tuneable:

* [`step_rm()`](https://tidymodels.github.io/recipes/reference/step_rm.html){target="_blank"}:
    Manually remove unused columns.
* [`step_nzv()`](https://tidymodels.github.io/recipes/reference/step_nzv.html){target="_blank"}:
    Automatically filter near-zero varianced columns.
* [`step_string2factor()`](https://tidymodels.github.io/recipes/reference/step_string2factor.html){target="_blank"}:
    Manually convert to `factor` columns.
* [`step_downsample()`](https://tidymodels.github.io/recipes/reference/step_downsample.html){target="_blank"}:
    Downsampling step to balancing target's class distribution (**tuneable**).
* [`step_center()`](https://tidymodels.github.io/recipes/reference/step_center.html){target="_blank"}:
    Normalize the mean of `numeric` column(s) to zero (**tuneable**).
* [`step_scale()`](https://tidymodels.github.io/recipes/reference/step_scale.html){target="_blank"}:
    Normalize the standard deviation of `numeric` column(s) to one (**tuneable**).
* [`step_pca()`](https://tidymodels.github.io/recipes/reference/step_pca.html){target="_blank"}:
    Shrink `numeric` column(s) to several PCA components (**tuneable**).

---

### Designing your first preprocess recipes

1. Initiate a recipe using `recipe()`
    + Define your formula in the first argument.
    + Supply a template dataset in `data` argument.
2. Pipe to every needed `step_*()`--always remember to put every step in proper consecutive manner.
3. After finished with every needed `step_*()`, pipe to `prep()` function to train your recipe
    + It will automatically convert all `character` columns to `factor`;
    + If you used `step_string2factor()`, don't forget to specify `strings_as_factors = FALSE`
    + It will train the recipe to the specified dataset in the `recipe()`'s `data` argument;
    + If you want to train to other dataset, you can supply the new dataset to `training` argument, and set the `fresh` argument to `TRUE`

---

Let's see an example of defining a recipe:

```{r}
# define preprocess recipe from train dataset
rec <- recipe(attrition ~ ., data = training(splitted)) %>% 
  step_rm(employee_count, employee_number) %>%
  step_nzv(all_predictors()) %>% 
  step_string2factor(all_nominal(), -attrition) %>%
  step_string2factor(attrition, levels = c("yes", "no")) %>%
  step_downsample(attrition, ratio = 1/1, seed = 100) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_pca(all_numeric(), threshold = 0.85) %>%
  prep(strings_as_factors = FALSE)

# quick check
rec
```

---

There are two ways of obtaining the result from our recipe:

* [`juice()`](https://tidymodels.github.io/recipes/reference/juice.html){target="_blank"}:
    Extract preprocessed dataset from `prep()`-ed recipe. Normally, we will use this function to get preprocessed train dataset.

* [`bake()`](https://tidymodels.github.io/recipes/reference/bake.html){target="_blank"}:
    Apply a recipe to new dataset. Normally, we use we will use this function to preprocess new dataset, such as test dataset, or prediction dataset.
    
---

These are some example on how to get preprocessed train and test dataset:

```{r}
# get preprocessed train dataset
data_train <- juice(rec)

# quick check
data_train

# get preprocessed test dataset
data_test <- bake(rec, testing(splitted))

# quick check
data_test
```

# Model Fitting using `parsnip`

## Rethinking: How many machine learning packages you used?

## One vegeta.. I mean package to rule them all: `parsnip`

---

[`parsnip`](https://tidymodels.github.io/parsnip/){target="_blank"} is part of `tidymodels` that could help us in model fitting and prediction flows.

There are so many models supported by `parsnip`--as you could see in its [full model list](https://tidymodels.github.io/parsnip/articles/articles/Models.html){target="_blank"}. In this introduction, we will use random forest as an example model.

---

There are two part of defining a model that should be noted:

* **Defining model's specification**:
    In this part, we need to define the model's specification, such as `mtry` and `trees` for random forest, through model specific functions. For example, you can use [`rand_forest()`](https://tidymodels.github.io/parsnip/reference/rand_forest.html){target="_blank"} to define a random forest specification. Make sure to check [full model list](https://tidymodels.github.io/parsnip/articles/articles/Models.html){target="_blank"} to see every model and its available arguments.

* **Defining model's engine**:
    In this part, we need to define the model's engine, which determines the package we will use to fit our model. This part could be done using [`set_engine()`](https://tidymodels.github.io/parsnip/reference/set_engine.html){target="_blank"} function. Note that in addition to defining which package we want to use as our engine, we could also passing package specific arguments to this function.

---

This is an example of defining a random forest model using `randomForest::randomForest()` as our engine:

```{r}
# set-up model specification
model_spec <- rand_forest(
  mode = "classification",
  mtry = ncol(data_train) - 2,
  trees = 500,
  min_n = 15
)

# set-up model engine
model_engine <- set_engine(
  object = model_spec,
  engine = "randomForest"
)

# quick check
model_engine
```

---

To fit our model, we have two options:

* Formula interface
* X-Y interface

Note that some packages are behaving differently inside formula and x-y interface. For example, `randomForest::randomForest()` would convert all of our categorical variables into dummy variables in formula interface, but not in x-y interface.

---

Fit using formula interface using `fit()` function:

```{r}
# fit the model
model <- fit(
  object = model_engine,
  formula = attrition ~ .,
  data = data_train
)

# quick check
model
```

---

Or through x-y interface using `fit_xy()` function:

```{r}
# fit the model
model <- fit_xy(
  object = model_engine,
  x = select(data_train, -attrition),
  y = select(data_train, attrition)
)

# quick check
model
```

---

In this workflow, it should be relatively easy to change the model engine.

Let's try to fit a same model specification, but now using `ranger::ranger()`:

```{r}
# set-up other model engine
model_engine <- set_engine(
  object = model_spec,
  engine = "ranger",
  seed = 100,
  num.threads = parallel::detectCores() / 2,
  importance = "impurity"
)

# quick check
model_engine
```

---

Now let's try to fit the model, and see the result:

```{r}
# fit the model
model <- fit(
  object = model_engine,
  formula = attrition ~ .,
  data = data_train
)

# quick check
model
```

---

Notice that `ranger::ranger()` doesn't behave differently between `fit()` and `fit_xy()`:

```{r}
# fit the model
model <- fit_xy(
  object = model_engine,
  x = select(data_train, -attrition),
  y = select(data_train, attrition)
)

# quick check
model
```

---

To get the prediction, we could use `predict()` as usual--but note that it would return a tidied `tibble` instead of a `vector`, as in `type = "class"` cases, or a raw `data.frame`, as in `type = "prob"` cases.

In this way, the prediction results would be very convenient for further usage, such as simple recombining with the original dataset:

```{r}
# get prediction on test
predicted <- data_test %>% 
  bind_cols(predict(model, data_test)) %>% 
  bind_cols(predict(model, data_test, type = "prob"))

# quick check
predicted %>% 
  select(attrition, matches(".pred"))
```

# Model Evaluation using `yardstick`

## Rethinking: How do we measure the goodness of our model?

## It's always good to bring your own `yardstick`

---

[`yardstick`](https://tidymodels.github.io/yardstick/){target="_blank"} is part of `tidymodels` that could help us in calculating model performance metrics.

There are so many metrics available by `yardstick`--as you could see in its [full function references page](https://tidymodels.github.io/yardstick/reference/index.html){target="_blank"}. In this introduction, we will calculate some model performance metrics for classification task as an example.

---

There are two ways of calculating model performance metrics, which differ in its input and output:

* `tibble` approach:
    We pass a dataset containing the `truth` and `estimate` to the function, and it will return a `tibble` containing the results, e.g., [`precision()`](https://tidymodels.github.io/yardstick/reference/precision.html){target="_blank"} function.
* `vector` approach:
    We pass a vector as the `truth` and a vector as the `estimate` to the function, and it will return a `vector` which show the results, e.g., [`precision_vec()`](https://tidymodels.github.io/yardstick/reference/precision.html){target="_blank"} function.

Note that some function, like [`conf_mat()`](https://tidymodels.github.io/yardstick/reference/conf_mat.html){target="_blank"}, only accept `tibble` approach, since it is not returned a `vector` of length one.

---

Let's start by reporting the confusion matrix:

```{r}
# show confusion matrix
predicted %>% 
  conf_mat(truth = attrition, estimate = .pred_class) %>% 
  autoplot(type = "heatmap")
```

---

Now, to calculate the performance metrics, let's try to use the `tibble` approach--which also support `group_by`:

```{r}
# calculate accuracy
predicted %>% 
  accuracy(attrition, .pred_class)

# calculate accuracy by group
predicted %>% 
  group_by(department) %>% 
  accuracy(attrition, .pred_class) %>% 
  ungroup()
```

---

Or using `vector` approach, which is more flexible in general:

```{r}
# metrics summary
predicted %>% 
  summarise(
    accuracy = accuracy_vec(attrition, .pred_class),
    sensitivity = sens_vec(attrition, .pred_class),
    specificity = spec_vec(attrition, .pred_class),
    precision = precision_vec(attrition, .pred_class)
  )

# metrics summary by group
predicted %>% 
  group_by(department) %>% 
  summarise(
    accuracy = accuracy_vec(attrition, .pred_class),
    sensitivity = sens_vec(attrition, .pred_class),
    specificity = spec_vec(attrition, .pred_class),
    precision = precision_vec(attrition, .pred_class)
  ) %>% 
  ungroup()
```

---

Sometimes the model performance metrics could also improving the models final results. For example, through Receiver Operating Curve, we could assess the probability threshold effect to sensitivity and specificity:

```{r}
predicted %>% 
  roc_curve(attrition, .pred_yes) %>% 
  autoplot()
```

---

And, since it's returning a `tibble`, we could do further data wrangling to help us see it more clearly:

```{r}
# get roc curve data on test dataset
pred_test_roc <- predicted %>%
  roc_curve(attrition, .pred_yes)

# quick check
pred_test_roc
```

---

With some `ggplot2`:

```{r}
# tidying
pred_test_roc <- pred_test_roc %>% 
  mutate_if(~ is.numeric(.), ~ round(., 4)) %>% 
  gather(metric, value, -.threshold)

# plot sensitivity-specificity trade-off
p <- ggplot(pred_test_roc, aes(x = .threshold, y = value)) +
  geom_line(aes(colour = metric)) +
  labs(x = "Probability Threshold to be Classified as Positive", y = "Value", colour = "Metrics") +
  theme_minimal()
```

---

and `plotly` magic, it would be perfect:

```{r}
# convert to plotly
ggplotly(p)
```

# Dive Deeper: Summing-up the Baseline Workflow

As an exercise, create a baseline `tidymodels` workflow using `attrition` dataset. Since it is a baseline workflow, the main objective is to make it as simple as possible--yet, covering all the basic parts in machine learning model fitting.

## Cross-validation scheme

```{r, eval=FALSE}
# insert your solution here
```

## Data preprocess

```{r, eval=FALSE}
# insert your solution here
```

## Model fitting

```{r, eval=FALSE}
# insert your solution here
```

## Model evaluation

```{r, eval=FALSE}
# insert your solution here
```

# Intermediate Workflow: Hyperparameter Tuning

---

Please checkout: [https://github.com/bagasbgy/tidymodels-examples](https://github.com/bagasbgy/tidymodels-examples){target="_blank"} at `classification-rand_forest` branch.

# Learn-by-building: Hyperparameter Tuning an `xgboost` Model

---

With:

* `loss_reduction` held constant at `0.01`
* `learn_rate` held constant at `0.01`
* `sample_size` held constant at `0.8`

Choose best model which giving accuracy, sensitivity, and specificity above 70%!

# Happy Learning and Coding! `r emojifont::emoji("smile")`
